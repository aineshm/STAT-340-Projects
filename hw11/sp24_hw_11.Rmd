---
title: "Homework 11"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem #1. Guided k-fold CV exercise <small>9pts</small>

In this exercise, we will guide you through an exercise where you are asked to use k-fold cross validation to evaluate the performance of several models.

For this exercise we will use the "Swiss Fertility and Socioeconomic Indicators (1888)" dataset from the `datasets` package, which is loaded below. (To view the help page, run `?datasets::swiss` in your console). We will be using `Fertility` as our response variable.

```{r}
swiss = datasets::swiss
```


### Part a) Understanding/visualizing data

Read the help page and briefly "introduce" this dataset. Specifically, explain where the data comes from, what variables it contains, and why should people care about the dataset.

Produce one or some visualizations of the data. Do your best here to try to use your plots to help your viewer best understand the structure and patterns of this dataset. Choose your plots carefully and briefly explain what each plot tells you about the data.

```{r}
# Correlation matrix heatmap
library(ggplot2)
library(corrplot)
cor_matrix <- cor(swiss)
corrplot(cor_matrix, method = "color")

# Scatter plot matrix
pairs(swiss, panel = panel.smooth, main = "Scatter plot matrix of Swiss data")

# Bar plot for 'Catholic' variable
ggplot(swiss, aes(x = as.factor(Catholic > 50))) +
  geom_bar() +
  labs(x = "Majority Catholic (>50%)", y = "Count of Provinces") +
  ggtitle("Distribution of Catholic and Non-Catholic Provinces")
```


### Part b) Starting with basic lm

Compare a model with all predictors with no interactions with 2 other models of YOUR choice. Fit all 3 models, show their summary outputs, and briefly comment on which one you think might perform the best when used for future predictions and why.

```{r}
# Model with all predictors
model1 <- lm(Fertility ~ ., data = swiss)
summary(model1)

# Model with interaction
model2 <- lm(Fertility ~ Agriculture*Education + Examination + Catholic + Infant.Mortality, data = swiss)
summary(model2)

# Model with selected predictors
model3 <- lm(Fertility ~ Education + Catholic + Infant.Mortality, data = swiss)
summary(model3)

```


### Part c) Estimating MSE using CV

Now, we are going to actually estimate the MSE of each model with K-fold cross validation. First we're going to set a seed and import the `caret` package (it should be already installed since it's a prerequisite for many other packages, but if it's not for some reason, you can install it with `install.packages("caret")`)

```{r}
set.seed(1)
library(caret)
```

Next, use the following chunk, which already has `method` set to `lm`, `data` set to the `swiss` data set, and validation method set to use 5-fold CV, to estimate the MSE of each of your models. All you need to do is add in a formula for your model and repeat for all 3 models you have.

```{r,error=T}
set.seed(1)
#remove this comment when you work on your homework
model1 = train(Fertility ~ . , method="lm", data=swiss, trControl = trainControl(method="cv", number=5))
print(model1)

model2 = train(Fertility ~ Agriculture*Education + Examination + Catholic + Infant.Mortality, method="lm", data=swiss, trControl = trainControl(method="cv", number=5))
print(model2)

model3 = train(Fertility ~ Education + Catholic + Infant.Mortality, method="lm", data=swiss, trControl = trainControl(method="cv", number=5))
print(model3)
```

Once you have your models fitted, use `print( )` to show the summary statistics for each model. Report the RMSE for each model, which is the square root of the MSE. Which of these models performs the best? Which performed the worst? Do these results agree with your expectations?

Bonus: repeat the above step, using `trControl = trainControl(method="repeatedcv", number=5, repeats=3)` which repeats each CV analysis 3times and averages out each run to get a more stable estimate of the MSE. Compare the results with the unrepeated MSE estimates. How do they compare?


## Problem #2: More cars!  <small>4pts</small>

This `Auto` dataset, in the `Auto.csv` file contains measurements on over 300 cars. In this problem you will look at the effect of sample size and over-fitting. First load the data.

```{r}
Auto <- read.csv("Auto.csv", stringsAsFactors = TRUE)
```

The dataset contains the response variable `mpg` and 7 predictor variables:

* `cylinders`  - the number of engine cylinders
* `displacement` - engine displacement
* `horsepower` 
* `weight`
* `acceleration`
* `year`
* `origin` - there are Asian, US and European cars; indicator variables have been added to the dataset for European and US cars.

The following function pulls the model formula out of the reg subsets object. It will be used later in the code. Be sure to run this chunk to put the function into the environment.
```{r}
# id: model id
# object: regsubsets object
# data: data used to fit regsubsets
# outcome: outcome variable
get_model_formula <- function(id, object, outcome){
  # get models data
  models <- summary(object)$which[id,-1]
  # Get outcome variable
  #form <- as.formula(object$call[[2]])
  #outcome <- all.vars(form)[1]
  # Get model predictors
  predictors <- names(which(models == TRUE))
  predictors <- paste(predictors, collapse = "+")
  # Build model formula
  as.formula(paste0(outcome, "~", predictors))
}  
```

This function, performs best subset model selection on a dataset with a specified response variable. It returns a list of models - the models with the lowest RSS for each model size from 1 to p. It will be used below. (Note: because of the origin variable a modification was made to only fit models up to size 7, not 8. This is because for a small sample size if there are not Asian, US and European cars the model fit will not work if all variables are included due to linear dependence among predictors)
```{r}
library(leaps)
getModels <- function (dataset, responseVar){
  models <- regsubsets(reformulate(".",responseVar), data = Auto.subset, nvmax = ncol(dataset)-2);
  modelList <- list("formula")
  nModels <- length(summary(models))-1
  for(i in 1:nModels){
    modelList[[i]] <- get_model_formula(i, models, responseVar)
  }
  return(modelList)  
}

```

Now we will run some code to answer the questions below. We will simulate having a small sample of cars to work with and fit the linear model. You will notice that in order to average over errors the entire simulation is performed `NMC=50` times. You may modify this if you wish. The primary line you will modify is where the sample size is set.

```{r, warning=FALSE}
sampleSize <- 15  #You should edit this number

NMC <- 50 #number of replications of this simulation
nFolds <- 5  #k=5 for 5-fold CV
nModels <- 7 # we will look at a maximum model size (# predictors) of 7.

errors <- data.frame('fold' = as.factor(rep(1:nFolds, nModels*NMC)),
                     'rep' = rep(1:NMC, each=nFolds*nModels),
                     'model' = rep(1:nModels, rep(nFolds, nModels)),
                     'mse' = rep(0, nModels*nFolds*NMC))
for(k in 1:NMC){
  Auto.subset <- Auto[sample(nrow(Auto), sampleSize),]
  modelList <- getModels(Auto.subset, "mpg")
  
  #Cross Validation
  folds <- split(sample(1:nrow(Auto.subset)), as.factor(1:nFolds))
  for(i in 1:nFolds){
    validation <- Auto.subset[folds[[i]],]
    training <- Auto.subset[-folds[[i]],]
    for(j in 1:nModels){
      fit <- lm(modelList[[j]], data=training)
      predictions <- predict(fit, newdata = validation)
      errors[errors$rep==k & errors$fold==i  & errors$model ==j, 'mse'] = mean((predictions-validation$mpg)^2)
    }
  }
}
avg <- aggregate(.~model, data=errors, FUN="mean")
plot(y=sqrt(avg$mse), x=avg$model, xlab="model", ylab="root mean square error", type="l", main="Comparison of Model Error")
```

a. If the sample size is 15, what is the size of the preferred model?

> For a sample size of 15, you should run the given simulation and observe the RMSE across different model sizes. The preferred model size is typically where you observe a minimum in the RMSE, indicating the best balance between bias and variance.

b. If the sample size is larger, say 60, what is the size of the preferred model?

> ncreasing the sample size to 60 and running the same simulation will allow you to observe changes in the RMSE and potentially in the preferred model size. A larger sample size may support a more complex model without overfitting.

c. Now consider if you have a sample of size 200. Does your preferred model change?

> With a sample size of 200, the behavior of the RMSE as model complexity increases should be observed again. It's expected that with more data, a more complex model can be justified if it provides a lower RMSE without overfitting.


d. What is your general conclusion after looking at the effect of sample size on model size and model error?

> The general conclusion from this exercise would be that as sample size increases, you might be able to fit a more complex model (more predictors) without overfitting. However, it's crucial to monitor the RMSE as the model complexity increases with sample size, to ensure that you are not simply fitting the noise in your data. A larger sample size tends to give a more reliable estimate of the model's performance and can support more complex models, but there is a trade-off point where increasing model complexity no longer results in better predictive performance.



### Problem #3: Optimal K  <small>8pts; 2pts each</small>

Suppose the variable $Y=4 + 5X_1 + 8X_2 + \epsilon$ where $\epsilon \sim N(0, 2^2)$. Pretend this is the true model, but we don't know that - we are going to collect a random sample of size 40 and fit a linear model. We want to estimate the model error using $K-fold$ cross validation. In this problem we will figure out the optimal number of folds to get the best estimate of the model error $E(Y_{n+1}-\hat{Y}_{n+1})^2$.

We have to make a few assumptions to do this estimation. Let's suppose that $X_1 \sim N(3, 1^2)$ and $X_2 \sim N(1, .5^2)$. You can use the following function to simulate data:

```{r}
simulate.data <- function(n=40){
  X1 <- rnorm(n, 3, 1)
  X2 <- rnorm(n, 1, .5)
  eps <- rnorm(n, 0, 2)
  Y <- 4 + 5*X1 + 8*X2 + eps
  return(data.frame(Y,X1,X2))
}
```

### a. Estimate model error using Monte Carlo

Use Monte Carlo estimation to estimate the MSE of a linear model fit to a sample size of 40 using both predictors. On each MC repetition you should:

  i. generate a sample data set of size 40
  ii. fit a linear model using both X1 and X2 as predictors
  iii. simulate 1000 (or more) out of sample data points
  iv. calculate the square root of average squared error on the out of sample data points.


```{r}
NMC <- 1000
nUnseen <- 1000
Ehat <- 0 #an empty vector to store estimated 

for(i in 1:NMC){
  # generate a sample of size 40
  data <- simulate.data(40)
    
  # fit the linear model with X1 and X2 as predictors
  fit <- lm(Y ~ X1 + X2, data=data)
  
  # simulate unseen data
  unseen_data <- simulate.data(nUnseen)
  
  # calculate the square root of the average squared error on the out of sample data points
  predictions <- predict(fit, newdata=unseen_data)
  Ehat[i] <- sqrt(mean((predictions - unseen_data$Y)^2))

}
(modelError <- mean(Ehat))
```

### b. Estimating MSE using CV
Now we imagine we don't know the true model error, but instead we want to estimate it with K-fold validation. The following function can be used to perform K-fold validation to estimate root mean squared error

```{r, warning=FALSE}
kfoldCV <- function(K, formula, dataset, responseVar){
  #idx is a shuffled vector of row numbers
  idx <- sample(1:nrow(dataset))
  #folds partitions the row indices
  folds <- suppressWarnings(split(idx, as.factor(1:K)))
  #an empty vector to hold estimated errors
  errors <- vector("numeric", K) 
  for(k in 1:K){
    #split the data into training and testing sets
    training <- dataset[-folds[[k]],]
    testing <- dataset[folds[[k]],]
    #go through each model and estimate MSE
    #fit the model to the training data
    fit <- lm(formula = formula, data=training)
    #calculate the sqrt of average squared error on the testing data
    errors[k] <- sqrt (mean((predict(fit, newdata=testing)-testing[,responseVar])^2))
  }
  return(mean(errors))
}
```

The following code runs an estimation simulation to help you see what happens to the estimate of model error as the number of folds increases. 
We will consider 2,3,4,5,6,8,10,15,20,30 and 40-fold CV.

```{r}
NMC <- 50
Ks <- c(2,3,4,5,6,8,10,15,20,30,40)
nK <- length(Ks)
formula <- reformulate(c("X1","X2"),"Y")

errors <- data.frame('replicate'=rep(1:NMC, each=nK),
                     'k' = rep(Ks, NMC),
                     'error' = rep(0, NMC*nK))
for(i in 1:NMC){
  myData <- simulate.data(40)
  for(k in Ks){
    errors[errors$replicate==i & errors$k==k, 'error'] <- kfoldCV(k, formula, myData, 'Y')
  }
}
averageErrors <- aggregate(error ~k, data=errors, FUN="mean")

plot(error ~ k, data=errors, col=rgb(0,0,0,.5))
lines(error ~ k, data=averageErrors, col="red", lwd=3)
abline(h=modelError)
```

From the code and plot generated answer the following questions:

i. What happens to the estimate of model error as the number of folds increases?

> As the number of folds increases, the estimated model error decreases. This trend is evidenced by the decreasing line, meaning the average estimated error for the cross-validation decreases as the number of folds increases. This could be due to the model being tested on more data points, which typically leads to a more accurate estimate of the out-of-sample error.

ii. Knowing the true model error, what number of folds seems to give the most unbiased estimate of model error?

> It's a bit challenging to pinpoint exactly without numerical values on the horizontal line representing the true model error. However, the number of folds that brings the average estimated error (indicated by the red line) closer to the horizontal line would be considered the most unbiased. From the plot, this seems to occur as the number of folds gets closer to 40. Since we're dealing with a finite sample, the Leave-One-Out cross-validation (which is essentially what 40-fold CV is for a sample size of 40) typically yields an almost unbiased estimate but can have high variance.

iii. Besides the estimate being unbiased, what other consideration would you want to make when you consider the number of folds to choose?

> Besides the estimate being unbiased, you would want to consider the variance of the error estimate and the computational cost when choosing the number of folds. While increasing the number of folds tends to give a more accurate estimate of the model error, it also increases the computational workload. Moreover, with too many folds, the training sets become very similar to each other, and the variance of the estimate may increase. Generally, a balance is sought where the bias is low, the variance is not too high, and the computational cost is manageable. Common choices in practice are 5-fold or 10-fold cross-validation, as they are considered to provide a good trade-off between these factors.


### c. The tradeoff
Finally look at this plot:
```{r}
vars <- aggregate(error ~k, data=errors, FUN="var")$error
bias2 <- (averageErrors$error-2.07)^2
errors$errorsq <- (errors$error-2.07)^2
mse <- aggregate(errorsq ~ k, data=errors, FUN="mean")$errorsq

plot(x=Ks, y=vars, ylim=c(0,max(mse)), type="l", ylab="")
lines(x=Ks, y=bias2, lty=2, col="blue")
lines(x=Ks, y=mse, lty=3, lwd=2, col="red")
```

i. What does the solid black line represent? What pattern/trend do you see?

> The solid black line likely represents the variance of the estimated model error as a function of the number of folds used in the cross-validation. The pattern observed here is that the variance initially decreases with an increasing number of folds, but starts to increase after a certain point. This pattern is typical because, with too few folds, the variance is high due to the smaller size of the validation set. As the number of folds increases, the validation sets become larger, leading to a reduction in variance. However, with too many folds (approaching Leave-One-Out CV), the variance can increase because each validation set consists of almost all the data except one observation, leading to high sensitivity to individual data points.


ii. What does the dotted blue line represent? What pattern/trend do you observe?

> The dotted blue line probably represents the squared bias. The pattern here shows that the squared bias decreases as the number of folds increases. This trend is consistent with the notion that as each training set in the cross-validation process becomes more representative of the full dataset (which occurs as the number of folds increases), the bias tends to decrease. However, when K is too large, overfitting can occur, and bias might start to increase slightly.



iii. What does the dotted red line represent? What pattern/trend do you observe?

> The dotted red line is likely representing the mean squared error (MSE), which is the sum of variance and squared bias. The trend observed is that the MSE initially decreases with the number of folds, finds a minimum, and then starts to increase again. This is the typical behavior showing the trade-off between bias and variance. The point at which the MSE is at its lowest is the optimal number of folds, where the cross-validation estimate of the model error is most accurate.



### d. Wrapping it up
Finally after all of this analysis, what number of folds would you conclude provides the best estimate of model error?

> Approximately 10 folds could give us the best estimate for the model error since it has the least variance, mean squared error, and squared bias error.



## Problem #4. Variable selection with `Carseats` <small>9pts (4 and 5)</small>

This question should be answered using the `Carseats` dataset from the `ISLR` package. If you do not have it, make sure to install it.

```{r}
library(ISLR)

Carseats = ISLR::Carseats

# you should read the help page by running ?Carseats
# we can also peek at the data frame before using it
str(Carseats)
head(Carseats)
```


### Part a) Visualizing/fitting

First, make some visualizations of the dataset to help set the stage for the rest of the analysis. Try to pick plots to show that are interesting informative.

```{r}

ggplot(Carseats, aes(x = Urban, y = Sales, fill = Urban)) +
  geom_bar(stat = "summary", fun = "sum", position = "dodge") +
  labs(x = "Urban", y = "Total Sales", title = "Bar Plot of Total Sales by Urban vs. Non-Urban") +
  scale_fill_manual(values = c("blue", "red")) +
  scale_x_discrete(labels = c("Yes" = "Urban", "No" = "Non-Urban"))


ggplot(Carseats, aes(x = ShelveLoc, y = Sales, fill = ShelveLoc)) +
  geom_boxplot() +
  labs(x = "ShelveLoc", y = "Sales") +
  ggtitle("Box Plot of Sales by ShelveLoc")

ggplot(Carseats, aes(x = US, y = Income, fill = US)) +
  geom_boxplot() +
  labs(x = "Country", y = "Income") +
  scale_x_discrete(labels = c("Yes" = "US", "No" = "Non US")) +
  scale_fill_manual(values = c("Yes" = "green", "No" = "orange")) +
  ggtitle("Box Plot of Income by Country") 

ggplot(Carseats, aes(x = Price, y = Sales)) +
  geom_point(color = "darkblue") +
  labs(x = "Price", y = "Sales", title = "Scatter Plot of Sales vs. Price")

ggplot(Carseats, aes(x = Advertising, y = Sales, color = ShelveLoc)) +
  geom_point() +
  labs(x = "Advertising Budget", y = "Sales", title = "Scatter Plot of Sales vs. Advertising")
```

Using some variable selection method (stepwise, LASSO, ridge, or just manually comparing a preselected of models using their MSEs), choose a set of predictors to use to predict `Sales`. Try to find the best model that you can that explains the data well and doesn't have useless predictors. Explain the choices you made and show the final model.

```{r}
stepwise_model <- step(lm(Sales ~ ., data = Carseats), direction = "both", trace = FALSE)
summary(stepwise_model)
```


### Part b) Interpreting/assessing model

According to your chosen model, Which predictors appear to be the most important or significant in predicting sales? Provide an interpretation of each coefficient in your model. Be careful: some of the variables in the model are qualitative!

```{r}

cat("Intercept (5.475226): This is the expected sales value when all other predictors are zero. Since having zero in some of the predictors does not make sense (e.g., price or income cannot be zero), the intercept is more of a statistical artifact in this context.

CompPrice (0.092571): For each one-unit increase in the competitor's price, sales are expected to increase by approximately 0.093 units (likely thousands of units), holding all other variables constant. This positive coefficient suggests that as competitor prices rise, the Carseats' sales tend to increase, potentially due to customers switching to the cheaper option.

Income (0.015785): For each one-unit increase in the consumer income (likely measured in thousands of dollars), sales are expected to increase by approximately 0.016 units. This indicates that higher income levels may lead to higher sales of car seats.

Advertising (0.115903): For each additional thousand dollars spent on advertising, sales are expected to increase by approximately 0.116 units. This implies a positive relationship between advertising expenditure and sales.

Price (-0.095319): For each one-unit increase in the price of the car seats, sales are expected to decrease by approximately 0.095 units. This negative relationship indicates that higher prices might deter customers, leading to lower sales.

ShelveLocGood (4.835675) and ShelveLocMedium (1.951993): These coefficients are compared to the base category (likely ShelveLocBad, which is not shown). So, sales are expected to be approximately 4.836 units higher for a good shelf location and 1.952 units higher for a medium shelf location than for a bad shelf location. This shows the significance of product placement on sales.

Age (-0.046128): For each one-year increase in the age of the children for whom the car seats are bought, sales are expected to decrease by approximately 0.046 units. This might suggest that products targeted at younger children have higher sales.

Residual Standard Error: An RSE of 1.019 on 392 degrees of freedom indicates the typical deviation of the actual sales from the predicted sales.

Multiple R-Squared (0.872): This suggests that about 87.2% of the variability in sales is explained by the model.

Adjusted R-Squared (0.8697): This is a modified version of R-squared that has been adjusted for the number of predictors in the model. It provides a more accurate measure of the goodness of fit, especially when comparing models with a different number of predictors.

F-Statistic: An F-statistic of 381.4 and the associated p-value of less than 2.2e-16 strongly suggest that the model is statistically significant—that is, at least one of the predictors is likely to be related to sales.

All predictors appear significant at the 0.001 level (indicated by \"***\") in predicting sales, which suggests that they all contribute meaningfully to the model.")

```

Estimate the out of sample MSE of your model and check any assumptions you made during your model fitting process. Discuss any potential model violations. How satisfied are you with your final model?

```{r}

# Estimating the out of sample MSE
set.seed(123) # For reproducibility
library(caret)
# Split the data into training and testing sets
index <- createDataPartition(Carseats$Sales, p = 0.8, list = FALSE)
train_data <- Carseats[index, ]
test_data <- Carseats[-index, ]
# Fit the model on the training data
final_model <- lm(Sales ~ ., data = train_data)
# Predict on the test data
predictions <- predict(final_model, newdata = test_data)
# Calculate MSE
mse <- mean((predictions - test_data$Sales)^2)

# Checking model assumptions with diagnostic plots
plot(final_model)

```


