---
title: "Homework 9"
author: Ainesh Mohan
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 1 More regression with `mtcars` (12 points; 2 pts each)

In lecture, we worked briefly with the `mtcars` data set.
Let's get more regression practice by working with it some more.

### a) background

Run `?mtcars` in the console (please __do not__ add it to this `Rmd` file) and briefly read the help page.
Specifically, take note of the following:

1. What is the source of this data?
2. What is this data set measuring (i.e., what was the response variable in the original study, at least based on the brief description in the R documentation)?
3. What predictors are available and what do they mean?

***

1. Source of the Data: The data was extracted from the 1974 Motor Trend US magazine.

2. Measurements: The dataset comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973â€“74 models). The primary response variable in the original study appears to be miles per gallon (mpg), which measures fuel efficiency.

3. Predictors Available: mpg: Miles per (US) gallon; 
cyl: Number of cylinders; 
disp: Displacement (cubic inches); 
hp: Gross horsepower; 
drat: Rear axle ratio; 
wt: Weight (1,000 lbs); 
qsec: 1/4 mile time (time it takes to travel a quarter mile from a standstill); 
vs: Engine shape (0 = V-shaped, 1 = straight); 
am: Transmission (0 = automatic, 1 = manual); 
gear: Number of forward gears; 
carb: Number of carburetors

***

You may want to also run `head(mtcars, 10)` or `View(mtcars)` to inspect the data frame briefly before moving on.

### b) Fitting a model

Use `lm` to run a regression of `mpg` on a few predictors in the data frame (choose two or three that you think would make a good model-- don't use all ten; we'll talk about why in later lectures).
Make sure to include `data = mtcars` as a keyword argument to `lm` so that R knows what data frame to use.

```{r}
lm.mtcars = lm(mpg ~ disp + hp + wt, data = mtcars)
```

Briefly inspect the residuals plot by running `plot(lm.mtcars,ask=F,which=1:2)`.
What do you observe, and what does it mean?

***

The residual patterns suggest that the assumptions of linear regression may not be fully met. The non-random pattern in the Residuals vs Fitted plot hints at a non-linear relationship, while the deviations in the Q-Q plot suggest possible issues with normality, which could affect the reliability of significance tests. Outliers or influential observations are present and should be further investigated to see if they are having a disproportionate effect on the model. Adjustments such as transforming variables, adding interaction terms, or considering different modeling approaches might be needed to improve the model fit.

***

### c) Interpreting the model

View the summary of your model by uncommenting and running the code below.
```{r}
summary(lm.mtcars)
```

Pick one of your predictors and give an interpretation of the estimate and standard error for its coefficient.
Be careful in your wording of the interpretation.

***

Coefficient (Estimate): The coefficient for wt is -3.80091. This means that for each 1,000-pound increase in the weight of the car, the model predicts a decrease in fuel efficiency (miles per gallon, mpg) by approximately 3.80091 units, holding all other variables constant.

Standard Error: The standard error of the coefficient for wt is 1.066191. This value measures the average amount that the coefficient estimate varies with different samples. A smaller standard error suggests that the estimate of the coefficient is more precise.

The t-value and the corresponding p-value indicate that wt is statistically significant in predicting the mpg (since the p-value is much less than 0.05).

So, in summary, the weight of a car is significantly inversely associated with its fuel efficiency, and the estimate of this effect is precise.

***

Which coefficients are statistically significantly different from zero? How do you know?

***

The intercept (37.105505), with a p-value < 2e-16 (which is essentially 0).
hp (horsepower), with a p-value of 0.01097.
wt (weight), with a p-value of 0.00133.

We consider a coefficient to be statistically significantly different from zero if the p-value is below a certain threshold, commonly 0.05 for a 95% confidence level.

***

### d) Interpreting residuals

What is the Residual Standard Error (RSE) for this model? How many degrees of freedom does it have?

***

RSE = 2.639; 28 degrees of freedom

***

What is the value of $R^2$ for this model? (__Hint:__ look at the output of `summary`) Give an interpretation of this value.

***


An $R^2$ of 0.8268 means that approximately 82.68% of the variability in the miles per gallon (mpg) can be explained by the linear relationship with the car's displacement, horsepower, and weight. This is a relatively high $R^2$, suggesting that the model fits the data well and that these predictors are good at explaining the variation in fuel efficiency among the cars in the dataset.

***

### e) Adjusted $R^2$

Briefly read about the adjusted $R^2$ [here](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/adjusted-r2/).
What is the adjusted $R^2$ of this model and how does this differ from the usual $R^2$ value? (__Hint:__ again, look at the output of `summary`).

***

Adj $R^2$ is 0.8083. Adjusted $R^2$ differs from the usual $R^2$ in that it accounts for the number of predictors in the model relative to the number of observations. While $R^2$ will always increase when more predictors are added to a model, whether they are significant or not, the adjusted $R^2$ may decrease if the additional predictors do not improve the model enough to offset the penalty for the increased complexity. 

***

### f) CIs for coefficients

Read the documentation for the `confint` function, and use it to generate $95\%$ confidence intervals for the coefficients of your model.
Give an interpretation of these confidence intervals.

```{r}
confint(lm.mtcars)
```

***

Intercept: The model predicts that, when disp, hp, and wt are all zero, the expected mpg will be between 32.78 and 41.43 with 95% confidence. Of course, it's not realistic to have these predictors at zero, but this intercept forms the baseline of the model.

disp: Since the confidence interval for disp includes zero (-0.02214 to 0.02026), we cannot be 95% confident that the displacement (disp) has a significant effect on the miles per gallon (mpg), as the interval crosses the point of no effect.

hp: The confidence interval for horsepower (hp) ranges from -0.05459 to -0.00773, not containing zero, suggesting that with 95% confidence, we can say that an increase in horsepower is associated with a decrease in fuel efficiency. Specifically, for each additional unit increase in horsepower, the mpg decreases between 0.00773 and 0.05459 miles per gallon, holding other factors constant.

wt: The confidence interval for weight (wt) is from -5.98483 to -1.61690, which also does not include zero. This means that with 95% confidence, we can say that an increase in weight is significantly associated with a decrease in mpg. For each 1,000-pound increase in weight, the mpg decreases between 1.617 and 5.985 miles per gallon, holding other factors constant.

***


## Problem 2) the `cats` data set (8 points; 2pts each)

The `cats` data set, included in the `MASS` library, contains data recorded from 144 cats.
Each row of the data set contains the body weight (`Bwt`, in kgs), heart weight (`Hwt`, in grams) and the sex (`Sex`, levels `'F'` and `'M'`) for one of the cats in the data set.

### a) plotting the data

Create a scatter plot showing heart weight on the y-axis and body weight on the x-axis.
Ignore the `Sex` variable in this plot.

```{r}
library(MASS)
head(cats)
```

```{r}

plot(cats$Bwt, cats$Hwt, 
     xlab = "Body Weight (kg)", 
     ylab = "Heart Weight (g)", 
     main = "Scatter Plot of Heart Weight vs Body Weight")

```

Briefly describe what you see. Is there a clear trend in the data?

> 
The scatter plot of the cats dataset shows a positive correlation between body weight and heart weight, with an apparent linear trend indicating that as cats increase in body weight, their heart weight tends to increase as well. The data points are more densely clustered at the lower end of body weight, suggesting that there's more variability in heart weight among lighter cats. There's no indication of outliers that might skew the analysis, and the relationship seems consistent across the observed range. Further analysis, potentially including sex-based differentiation or regression modeling, could provide more insights into this relationship.

### b) fitting a linear model

Fit a linear regression model to predict cat heart weight from cat body weight (and using an intercept term, of course).

```{r}

lm.cats <- lm(Hwt ~ Bwt, data = cats)

summary(lm.cats)
```

Examine the coefficients of your fitted model.
What is the coefficient for the `Bwt` variable?
Interpret this coefficient-- a unit change in body weight yields how much change in heart weight?

```{r}

# TODO: additional code (if needed to extract coefficients) here

```

***

Coeff = 4.0341

Interpreting this coefficient, it means that for every additional kilogram of body weight, a cat's heart weight is expected to increase by 4.0341 grams on average. The coefficient is statistically significant, as indicated by the p-value being less than 2e-16 (which is effectively zero), meaning that we can be very confident that the true effect of body weight on heart weight is not zero.

***

### c) back to plotting

Create the same plot from Part a above, but this time color the points in the scatter plot according to the `Sex` variable.
You may use either `ggplot2` or the built-in R plotting tools, though I would recommend the former, for this.

You should see a clear pattern. Describe it. A sentence or two is fine here.

```{r}

library(ggplot2)

ggplot(cats, aes(x = Bwt, y = Hwt, color = Sex)) +
  geom_point() +
  labs(x = "Body Weight (kg)", y = "Heart Weight (g)", 
       title = "Scatter Plot of Heart Weight vs Body Weight by Sex") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red"))

```

***

The scatter plot shows a clear pattern where male cats (in red) tend to have both higher body weights and heart weights compared to female cats (in blue). Male cats are generally represented by points that are further to the right (indicating heavier body weight) and higher up (indicating heavier heart weight) on the plot. The data for female cats cluster at the lower end of both body weight and heart weight. This separation suggests a distinct difference in body and heart weights between the sexes.

***

### d) adding `Sex` and an interaction

From looking at the data, it should be clear that the `Sex` variable has explanatory power in predicting heart weight, but it is also very correlated with body weight.

Fit a new linear regression model, still predicting heart weight, but this time including both body weight and sex as predictors *and* an interaction term between body weight and sex.
Take note of how R assigns `Sex` a dummy encoding.

```{r}

lm.cats.sex <- lm(Hwt ~ Bwt * Sex, data = cats)

# View the summary of the model
summary(lm.cats.sex)

```

Examine the outputs of your model.
In particular, note the coefficients of `Sex` and the interaction between `Bwt` and `Sex`.
Are both of these coefficients statistically significantly different from zero?
How do you interpret the interaction term?

***


The linear regression model predicts heart weight from body weight and sex, including an interaction between body weight and sex. The main effect of sex suggests that at zero body weight (a theoretical reference point), male cats have a lower heart weight compared to female cats. More importantly, the interaction term is statistically significant, indicating that the effect of body weight on heart weight is different for male cats compared to female cats. Specifically, as body weight increases, the increase in heart weight is greater for male cats by 1.6763 grams for each kilogram of body weight. This suggests that the relationship between body weight and heart weight is not merely additive but varies by sex.

***


## Problem 3 - Using Multiple regression to fit nonlinear data (10 points, 2.5 pts each)

Open the dataset `multData.csv`. This data set consists of three predictor variables, simply named `X1`, `X2` and `X3`. The response variable is `Y`. In this problem you will explore how to use the multiple regression model to model nonlinear relationships.

### a) the first model

First we will explore the relationship between $Y$ and the first two predictors $X1$ and $X2$. Fit the linear model

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$
Interpret the coefficients of both X1 and X2. 

```{r}

# Read the data
multData <- read.csv("../../data/multData.csv")

# Fit the linear model Y ~ X1 + X2
model1 <- lm(Y ~ X1 + X2, data = multData)

# View the summary of the model
summary(model1)

```

***

Intercept: The estimated intercept is 860.5679, which would be the predicted value of Y when both X1 and X2 are zero. This value has a practical interpretation only if zero is within the range of plausible values for X1 and X2.

X1: The coefficient for X1 is -6.7573, which means that for each unit increase in X1, the value of Y is expected to decrease by 6.7573 units, assuming X2 is held constant. This relationship is statistically significant, as indicated by the very small p-value (< 2e-16), which is practically zero. The negative sign of the coefficient indicates an inverse relationship between X1 and Y.

X2: The coefficient for X2 is -22.8693. This indicates that for each unit increase in X2, the value of Y is expected to decrease by 22.8693 units, assuming X1 is held constant. This effect is also statistically significant, with a p-value of 3.91e-08, which is much less than the typical significance level of 0.05.

The negative coefficients suggest that both X1 and X2 have a negative linear relationship with Y.

***


### b) Investigating interaction of quantitative predictors

Next introduce an interaction term to the model
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2 + \epsilon$$

Fit the model and view the summary output. Has this improved the model fit? Did anything surprising happen to the coefficients? Try to explain what happened.


```{r}

# Fit the linear model with interaction term Y ~ X1 + X2 + X1:X2
model2 <- lm(Y ~ X1 + X2 + X1:X2, data = multData)

# View the summary of the model
summary(model2)

```

***

The multiple R-squared has increased slightly from 0.9758 to 0.9827, and the adjusted R-squared has increased from 0.9748 to 0.9816. This suggests that the interaction term provides additional explanatory power and improves the fit of the model.
The residual standard error has decreased from 6.602 to 5.643, indicating better predictive accuracy with the interaction term included.

The change from negative to positive coefficients for X1 and X2 suggests that their relationship with Y is not simply linear. Instead, the effect of each predictor on Y depends on the value of the other predictor.

***


### c) Introducing the last predictor

Next fit a model that introduces the `X3` variable. 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2  + \beta_4 X_3 \epsilon$$
Has the model fit improved? In what way (Justify your answer)? 

```{r}

# Fit the linear model including X3
model3 <- lm(Y ~ X1 + X2 + X1:X2 + X3, data = multData)

# View the summary of the model
summary(model3)

```

***

Residual Standard Error (RSE): The RSE has dropped significantly from 5.643 in the previous model to 1.077 in this model. A lower RSE indicates that the model's predictions are, on average, closer to the actual observed values.

R-squared: The R-squared has increased from 0.9827 to 0.9994. This value is very close to 1, suggesting that the model explains almost all the variability in the response variable Y.

Adjusted R-squared: Similar to R-squared, the adjusted R-squared has also increased, from 0.9816 to 0.9993. This increase is important because the adjusted R-squared accounts for the number of predictors in the model, providing a more accurate measure when adding more variables.

***


### d) Considering higher order terms

Finally explore higher order terms for the X3 variable: Introduce $X3^2$, $X3^3$ etc and determine if any of these higher order terms are justified in the model. Explain your reasoning and present your final model. Look at the diagnostic plots and discuss whether the assumptions of the multiple regression model seem to be justified.

```{r}

# Fit the model with higher order terms for X3
model4 <- lm(Y ~ X1 + X2 + X1:X2 + X3 + I(X3^2) + I(X3^3), data = multData)

# View the summary of the model
summary(model4)

```

***

Neither $X3^2$ nor $X3^3$ are statistically significant, as their p-values are 0.805 and 0.886, respectively, which are well above any conventional significance level (e.g., 0.05 or 0.01).
The coefficients for $X3^2$ and $X3^3$ are also quite large, but the large standard errors suggest that the estimates are not precise, and the true effect could be very different.
Given the lack of statistical significance and the large standard errors, these higher-order terms are not justified for inclusion in the model. They do not appear to provide additional explanatory power that is useful or reliable.

Final model:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2  + \beta_4 X_3 \epsilon$$


***
